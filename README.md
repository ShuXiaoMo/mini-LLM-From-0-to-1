# mini-LLM-From-0-to-1
Build a mini-LLM from 0 to 1, and find mathematica in AI.

## News
[2026/1/22] We release the script -- RMSNorm_temp.ipynb.

[2026/1/21] The repository was built!

## Recent Plans
### 1 Base
1.1 LayerNorm&RMSNorm(Layer Normalization&Root Mean Square Normalization);

1.2 RoPE(Rotary Position Embedding)&YaRN;

1.3 MHA&GQA(Multi Head Attention&Grouped Query Attention);

1.4 Causal Mask;

1.5 KV cache;

1.6 FeedForward&MoE;

1.7 Overview of the Model.

## References
[1] https://github.com/jingyaogong/minimind

[2] Vaswani A, Shazeer N, Parmar N, et al. Attention Is All You Need[EB/OL]. arXiv:1706.03762, 2017.

